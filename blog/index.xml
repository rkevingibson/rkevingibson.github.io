<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blog on Kevin Gibson&#39;s personal site</title>
    <link>https://rkevingibson.github.io/blog/</link>
    <description>Recent content in Blog on Kevin Gibson&#39;s personal site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Dec 2018 18:46:36 -0800</lastBuildDate><atom:link href="https://rkevingibson.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Neural networks as Ordinary Differential Equations</title>
      <link>https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/</link>
      <pubDate>Tue, 11 Dec 2018 18:46:36 -0800</pubDate>
      
      <guid>https://rkevingibson.github.io/blog/neural-networks-as-ordinary-differential-equations/</guid>
      <description>Recently I found a paper being presented at NeurIPS this year, entitled Neural Ordinary Differential Equations, written by Ricky Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud from the University of Toronto. The core idea is that certain types of neural networks are analogous to a discretized differential equation, so maybe using off-the-shelf differential equation solvers will help get better results. This led me down a bit of a rabbit hole of papers that I found very interesting, so I thought I would share a short summary/view-from-30,000 feet on this idea.</description>
    </item>
    
  </channel>
</rss>
